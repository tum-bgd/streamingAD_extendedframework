{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f811d0cc-6348-4136-a5e6-3f189882c952",
   "metadata": {},
   "source": [
    "# Difference in nonconformity and anomaly scores\n",
    "This script evaluates anomaly detection algorithms based on their capability to differentiate between normal and anomalous segments in a time series. For this purpose, the nonconformity and anomaly scores are analyzed for an anomalous segment, compared to the average score level directly before the anomaly. Anomalies can be labeled anomalies found in benchmark datasets or artificially introduced anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f9c50c1-9631-435d-bd16-a2b1b098ed50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae17cdc-0bbc-4299-9601-435b72c0363a",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dba88e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "buffer_anomaly = 25\n",
    "buffer_between_interval_anomaly = 100\n",
    "preceding_anomaly_interval_length = 50\n",
    "preceding_offset = preceding_anomaly_interval_length + buffer_between_interval_anomaly + buffer_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64456b69-2f63-4474-8f35-b6359a6b4537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_representation_length, dataset_category = 100, 'multivariate'\n",
    "collection_id = 'SMD'\n",
    "tuning_job_name = 'streaming-SMD-240306-2309'\n",
    "output_folder_path = f'../out/{dataset_category}/{collection_id}/{tuning_job_name}/{collection_id}'\n",
    "data_folder_base_path = f'../data/{dataset_category}/{collection_id}'\n",
    "dataset_ids = sorted([x for x in os.listdir(output_folder_path) if not x.startswith('.') and not x.endswith('.json')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec862f33-68f0-4880-b173-12a7c5594b85",
   "metadata": {},
   "source": [
    "## Analyze nonconformity scores after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89efe8f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_anomaly_sequences(target):\n",
    "    anomaly_sequences = []\n",
    "    anomaly_indices = np.unique(np.where(target == 1)[0])\n",
    "    change_ind = np.where(np.diff(anomaly_indices) != 1)[0] + 1\n",
    "    if len(change_ind) != 0:\n",
    "        sequences = np.split(anomaly_indices, change_ind)\n",
    "    else:\n",
    "        sequences = [anomaly_indices]\n",
    "    for sequence in sequences:\n",
    "        if len(sequence) != 0:\n",
    "            anomaly_sequences.append([np.min(sequence), np.max(sequence)])\n",
    "    return anomaly_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5eb0b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def overlap(start1, end1, start2, end2):\n",
    "    \"\"\"Does the range (start1, end1) overlap with (start2, end2)?\"\"\"\n",
    "    return not (end1 < start2 or end2 < start1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e90b45ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now at dataset machine-1-1\n",
      "Now at dataset machine-1-2\n",
      "Now at dataset machine-1-3\n",
      "Now at dataset machine-1-4\n",
      "Now at dataset machine-1-5\n",
      "Now at dataset machine-1-6\n",
      "Now at dataset machine-1-7\n",
      "Now at dataset machine-1-8\n",
      "Now at dataset machine-2-1\n",
      "Now at dataset machine-2-2\n",
      "Now at dataset machine-2-3\n",
      "Now at dataset machine-2-4\n"
     ]
    }
   ],
   "source": [
    "results_total = {}\n",
    "for dataset_id in dataset_ids:\n",
    "    print(f\"Now at dataset {dataset_id}\")\n",
    "    results = {}\n",
    "    test_data = pd.read_csv(f'{data_folder_base_path}/{dataset_id}.test.csv')\n",
    "    labels = test_data['is_anomaly'].to_numpy()\n",
    "    if len(np.unique(labels)) == 1:\n",
    "        continue\n",
    "    true_anomaly_sequences = calculate_anomaly_sequences(labels)\n",
    "    approaches_paths = [x for x in os.listdir(f'{output_folder_path}/{dataset_id}') if not x.startswith('.') and not x in ['initial_weights', 'artificial_anomalies']]\n",
    "    for approach_path in approaches_paths:\n",
    "        run_date_id = [x for x in os.listdir(f'{output_folder_path}/{dataset_id}/{approach_path}') if not x.startswith('.')][0]\n",
    "        artificial_anomalies_path = f'{output_folder_path}/{dataset_id}/artificial_anomalies/{run_date_id}'\n",
    "        if os.path.exists(artificial_anomalies_path):\n",
    "            artificial_anomaly_sequences = [[int(x.group()) for x in re.finditer(r'\\d+', fn)] for fn in os.listdir(artificial_anomalies_path) if not fn.startswith('.')]\n",
    "        else:\n",
    "            artificial_anomaly_sequences = []\n",
    "        all_anomaly_sequences = artificial_anomaly_sequences + true_anomaly_sequences\n",
    "\n",
    "        model_id, learning_strategy_id, anomaly_score_id = approach_path.split('-')\n",
    "        if anomaly_score_id == 'confidence_levels':\n",
    "            continue\n",
    "        if model_id not in results.keys():\n",
    "            results[model_id] = {}\n",
    "        if learning_strategy_id not in results[model_id].keys():\n",
    "            results[model_id][learning_strategy_id] = {}\n",
    "        if anomaly_score_id not in results[model_id][learning_strategy_id].keys():\n",
    "            results[model_id][learning_strategy_id][anomaly_score_id] = []\n",
    "        score_path = f'{output_folder_path}/{dataset_id}/{approach_path}/{run_date_id}'\n",
    "        anomaly_scores = pd.read_csv(f'{score_path}/anomaly_scores.csv').to_numpy()\n",
    "        nonconformity_scores = pd.read_csv(f'{score_path}/nonconformity_scores.csv').to_numpy()\n",
    "        offset = int(anomaly_scores[0, 0])\n",
    "\n",
    "        # compare anomaly maximum nc/anomaly score to previous average (check that this sequence is not actually another previous anomaly) and save in results dict\n",
    "        for seq in all_anomaly_sequences:\n",
    "            start, end = seq[0] - buffer_anomaly - offset, seq[1] + 1 + buffer_anomaly - offset\n",
    "            if start - preceding_offset < 0:\n",
    "                continue\n",
    "            max_anomaly_score = anomaly_scores[start:end, 1].max()\n",
    "            max_nonconformity_score = nonconformity_scores[start:end, 1].max()\n",
    "            if not any([overlap(seq[0] - buffer_anomaly - preceding_offset, seq[0] - 1, seq2[0], seq2[1]) for seq2 in all_anomaly_sequences]):\n",
    "                # print(f'Anomaly sequence: {seq[0]} - {seq[1]}')\n",
    "                preceding_anomaly_mean = np.mean(anomaly_scores[start - preceding_offset : start - preceding_offset + preceding_anomaly_interval_length, 1])\n",
    "                preceding_nonconformity_mean = np.mean(nonconformity_scores[start - preceding_offset : start - preceding_offset + preceding_anomaly_interval_length, 1])\n",
    "            else:\n",
    "                # print(f'Skipping anomaly sequence: {seq[0]} - {seq[1]}')\n",
    "                continue\n",
    "            results[model_id][learning_strategy_id][anomaly_score_id].append({\n",
    "                \"anomaly_sequence\": str(seq),\n",
    "                \"is_artificial\": seq in artificial_anomaly_sequences,\n",
    "                \"preceding_anomaly_mean\": preceding_anomaly_mean,\n",
    "                \"max_anomaly_score\": max_anomaly_score,\n",
    "                \"anomaly_score_diff\": max_anomaly_score - preceding_anomaly_mean,\n",
    "                \"preceding_nonconformity_mean\": preceding_nonconformity_mean,\n",
    "                \"max_nonconformity_score\": max_nonconformity_score,\n",
    "                \"nonconformity_score_diff\": max_nonconformity_score - preceding_nonconformity_mean,\n",
    "            })\n",
    "\n",
    "    results_total[dataset_id] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "317157dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'{output_folder_path}/results_difference_nc_anomaly_scores.json', 'w') as file:\n",
    "    json.dump(results_total, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf077971",
   "metadata": {},
   "source": [
    "## Post Processing\n",
    "Calculate absolute and relative difference averaged over all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba094fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = {'as_abs': [], 'as_rel': [], 'nc_abs': [], 'nc_rel': []}\n",
    "template_outer = {\n",
    "    'artificial_anomalies': deepcopy(template),\n",
    "    'real_anomalies': deepcopy(template),\n",
    "    'all_anomalies': deepcopy(template)\n",
    "}\n",
    "results_post_processing = {\n",
    "    'models': {},\n",
    "    'learning_strategies': {},\n",
    "    'models_learning_strategies': {}\n",
    "}\n",
    "for dataset_id in results_total.keys():\n",
    "    approaches_paths = [x for x in os.listdir(f'{output_folder_path}/{dataset_id}') if not x.startswith('.') and not x in ['initial_weights', 'artificial_anomalies']]\n",
    "    for approach_path in approaches_paths:\n",
    "        model_id, learning_strategy_id, anomaly_score_id = approach_path.split('-')\n",
    "        model_ls_id = f'{model_id}-{learning_strategy_id}'\n",
    "        if anomaly_score_id == 'confidence_levels':\n",
    "            continue\n",
    "        if model_id not in results_post_processing['models'].keys():\n",
    "            results_post_processing['models'][model_id] = deepcopy(template_outer)\n",
    "        if learning_strategy_id not in results_post_processing['learning_strategies'].keys():\n",
    "            results_post_processing['learning_strategies'][learning_strategy_id] = deepcopy(template_outer)\n",
    "        if model_ls_id not in results_post_processing['models_learning_strategies'].keys():\n",
    "            results_post_processing['models_learning_strategies'][model_ls_id] = deepcopy(template_outer)\n",
    "        for dict_key, obj_id in [('models', model_id), ('learning_strategies', learning_strategy_id), ('models_learning_strategies', model_ls_id)]:\n",
    "            for anomaly_entry in results_total[dataset_id][model_id][learning_strategy_id][anomaly_score_id]:\n",
    "                if anomaly_entry['is_artificial']:\n",
    "                    anomaly_categories = ['artificial_anomalies', 'all_anomalies']\n",
    "                else:\n",
    "                    anomaly_categories = ['real_anomalies', 'all_anomalies']\n",
    "                for anomaly_category in anomaly_categories:\n",
    "                    results_post_processing[dict_key][obj_id][anomaly_category]['as_abs'].append(anomaly_entry['max_anomaly_score'] - anomaly_entry['preceding_anomaly_mean'])\n",
    "                    results_post_processing[dict_key][obj_id][anomaly_category]['as_rel'].append(anomaly_entry['max_anomaly_score'] / anomaly_entry['preceding_anomaly_mean'] - 1.0)\n",
    "                    results_post_processing[dict_key][obj_id][anomaly_category]['nc_abs'].append(anomaly_entry['max_nonconformity_score'] - anomaly_entry['preceding_nonconformity_mean'])\n",
    "                    results_post_processing[dict_key][obj_id][anomaly_category]['nc_rel'].append(anomaly_entry['max_nonconformity_score'] / anomaly_entry['preceding_nonconformity_mean'] - 1.0)\n",
    "                \n",
    "       \n",
    "# Average across categories\n",
    "for dict_key in results_post_processing.keys():\n",
    "    for obj_key in results_post_processing[dict_key].keys():\n",
    "        for anomaly_category in template_outer.keys():\n",
    "            for score_key in template.keys():\n",
    "                results_post_processing[dict_key][obj_key][anomaly_category][score_key] = sum(results_post_processing[dict_key][obj_key][anomaly_category][score_key]) / max(1, len(results_post_processing[dict_key][obj_key][anomaly_category][score_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5487ba68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'{output_folder_path}/results_post_processing_difference_nc_anomaly_scores.json', 'w') as file:\n",
    "    json.dump(results_post_processing, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c77f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b53ad4e6ecf90fd6d913cf6b107c2a117277284f0518724314a77102f61b9f56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
