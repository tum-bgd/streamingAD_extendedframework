{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602fa324-5751-4f98-b0a9-7c895513c0e2",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "This script calculates the performance of all approaches in an output folder. Following metrics are calculated:\n",
    "- PR AUC\n",
    "- Best Precision, Recall, F0.5\n",
    "- NAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b797a19-e0fd-45a7-a571-b8c29921b1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b758c54-f788-4ff5-b55e-b6ab6847ba81",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dd8c5b8-906b-420f-91ea-7c9daf5bded2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuning_job_name = 'streaming-SMD-240307-1040'\n",
    "dataset_category, collection_id, further = 'multivariate', 'SMD', f'/{tuning_job_name}/SMD'\n",
    "output_folder_path = f'../out/{dataset_category}/{collection_id}{further}'\n",
    "data_folder_base_path = f'../data/{dataset_category}/{collection_id}'\n",
    "dataset_ids = [x for x in os.listdir(output_folder_path) if not x.startswith('.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c37e76-b07c-4f83-b091-b69355970100",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3db240b-c3a1-4a93-aa69-5da44e28d344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_anomaly_sequences(target):\n",
    "    anomaly_sequences = []\n",
    "    anomaly_indices = np.unique(np.where(target == 1)[0])\n",
    "    change_ind = np.where(np.diff(anomaly_indices) != 1)[0] + 1\n",
    "    if len(change_ind) != 0:\n",
    "        sequences = np.split(anomaly_indices, change_ind)\n",
    "    else:\n",
    "        sequences = [anomaly_indices]\n",
    "    for sequence in sequences:\n",
    "        if len(sequence) != 0:\n",
    "            anomaly_sequences.append([np.min(sequence), np.max(sequence)])\n",
    "    return anomaly_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b587e69a-656f-4345-a742-8391949de174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_anomaly_sequences_2d(targets):\n",
    "    assert len(targets.shape) == 2   # first dimension is threshold dimension\n",
    "    anomaly_sequences = [[] for i in range(len(targets))]\n",
    "    sequences = []\n",
    "    for i in range(len(targets)):\n",
    "        anomaly_indices = np.where(targets[i] == 1)[0]\n",
    "        change_ind = np.where(np.diff(anomaly_indices) != 1)[0] + 1\n",
    "        if len(change_ind) != 0:\n",
    "            sequences.extend(np.split(np.stack([i * np.ones_like(anomaly_indices), anomaly_indices], axis=0), change_ind, axis=1))\n",
    "        else:\n",
    "            sequences.append(np.stack([i * np.ones_like(anomaly_indices), anomaly_indices], axis=0))\n",
    "    # print(f'There are {len(sequences)} sequences')\n",
    "    for sequence in sequences:\n",
    "        if sequence.size != 0:\n",
    "            threshold_idx = int(sequence[0, 0])\n",
    "            anomaly_sequences[threshold_idx].append([np.min(sequence[1, :]), np.max(sequence[1, :])])\n",
    "    return anomaly_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7a8688f-5244-406a-858a-1ccdfab50871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def overlap(start1, end1, start2, end2):\n",
    "    \"\"\"Does the range (start1, end1) overlap with (start2, end2)?\"\"\"\n",
    "    return not (end1 < start2 or end2 < start1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7837cb43-ebf1-4403-8877-cda8b66d815a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pr_auc(anomaly_scores, labels, return_best_threshold_precision_recall=True):\n",
    "    assert len(anomaly_scores.shape) == 1\n",
    "    thresholds = np.arange(0, 1, 0.01)\n",
    "\n",
    "    # 1) Calculate binary prediction for every threshold\n",
    "    predictions_binary = np.stack([anomaly_scores > thresholds[i] for i in range(len(thresholds))], axis=0)\n",
    "\n",
    "    # 2) Smoothing (optional)\n",
    "\n",
    "    # 3) Calculate anomaly sequences\n",
    "    pred_anomaly_sequences = calculate_anomaly_sequences_2d(predictions_binary)\n",
    "    true_anomaly_sequences = calculate_anomaly_sequences(labels)\n",
    "\n",
    "    # 4) Calculate TP, FN, FP for every threshold\n",
    "    true_positives, false_negatives, false_positives = [], [], []\n",
    "    precisions, recalls = [], []\n",
    "    for i in range(len(thresholds)):\n",
    "        TP, FN, FP = 0, 0, 0\n",
    "        pred_anomaly_sequences_single_threshold_copy = pred_anomaly_sequences[i].copy()\n",
    "        for true_sequence in true_anomaly_sequences:\n",
    "            # Check whether there is overlap with any pred_sequence\n",
    "            overlap_list = [(pred_sequence, overlap(true_sequence[0], true_sequence[1], pred_sequence[0], pred_sequence[1])) for pred_sequence in pred_anomaly_sequences_single_threshold_copy]\n",
    "            if any([x[1] for x in overlap_list]):\n",
    "                TP += 1\n",
    "                for pred_sequence in [x[0] for x in overlap_list if x[1]]:\n",
    "                    # print(f'{pred_sequence} overlapping with {true_sequence}')\n",
    "                    pred_anomaly_sequences_single_threshold_copy.remove(pred_sequence)\n",
    "            else:\n",
    "                FN += 1\n",
    "        FP = len(pred_anomaly_sequences_single_threshold_copy)\n",
    "        true_positives.append(TP)\n",
    "        false_negatives.append(FN)\n",
    "        false_positives.append(FP)\n",
    "        precisions.append(TP / max(1, TP + FP))\n",
    "        recalls.append(TP / max(1, TP + FN))\n",
    "\n",
    "    # 5) Calculate AUC\n",
    "    precisions, recalls = np.array(precisions), np.array(recalls)\n",
    "    # auc = np.sum(np.abs(recalls[1:] - recalls[:-1]) * precisions[1:])\n",
    "    sort_ind = np.argsort(recalls)\n",
    "    auc_score = auc(recalls[sort_ind], precisions[sort_ind])\n",
    "\n",
    "    # 6) Find best threshold\n",
    "    if return_best_threshold_precision_recall:\n",
    "        idx = np.argmax(np.square(precisions) + np.square(recalls))\n",
    "        return auc_score, thresholds[idx], precisions[idx], recalls[idx]\n",
    "    else:\n",
    "        return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7df10435-8608-4af2-9120-089a16120ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nab_scoring_function(y):\n",
    "    return 2 * (1 / (1 + np.exp(5*y))) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85b1d573-b948-4d55-98b2-4f78c6a8a553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nab_scoring(anomaly_scores, labels, return_threshold=True):\n",
    "    assert len(anomaly_scores.shape) == 1\n",
    "    thresholds = np.arange(0, 1, 0.01)\n",
    "\n",
    "    # 1) Calculate binary prediction for every threshold\n",
    "    predictions_binary = np.stack([anomaly_scores > thresholds[i] for i in range(len(thresholds))], axis=0)\n",
    "\n",
    "    # 2) Smoothing (optional)\n",
    "\n",
    "    # 3) Calculate true anomaly sequences\n",
    "    true_anomaly_sequences = calculate_anomaly_sequences(labels)\n",
    "\n",
    "    # 4) Enlarge true anomaly sequences (optional)\n",
    "\n",
    "    # 5) Calculate scores for each time steps based on NAB scoring function and true anomaly windows\n",
    "    scores_time_steps = -1. * np.ones((len(labels)))\n",
    "    for true_sequence in true_anomaly_sequences:\n",
    "        start, end = true_sequence[0], true_sequence[1]\n",
    "        len_seq = end - start\n",
    "        for i in range(2*len_seq):\n",
    "            scores_time_steps[min(start + i, len(labels) - 1)] = nab_scoring_function(3 * (i / len_seq) - 3)\n",
    "\n",
    "    # 6) Sum all scores for every threshold\n",
    "    scores_time_steps = np.stack([scores_time_steps for i in range(len(thresholds))], axis=0)\n",
    "    # return scores_time_steps, predictions_binary\n",
    "    scores_thresholds = scores_time_steps * predictions_binary\n",
    "    for true_sequence in true_anomaly_sequences:\n",
    "        start, end = true_sequence[0], true_sequence[1]\n",
    "        detections = scores_thresholds[:, start:end+1] > 0\n",
    "        first_detection_mask = detections.cumsum(axis=1).cumsum(axis=1) == 1\n",
    "        scores_thresholds[:, start:end+1][~first_detection_mask] = 0\n",
    "    scores_thresholds = np.sum(scores_thresholds, axis=1)\n",
    "\n",
    "    # 7) Normalize by the number of true anomaly sequences\n",
    "    if len(true_anomaly_sequences) != 0:\n",
    "        scores_thresholds /= len(true_anomaly_sequences)\n",
    "\n",
    "    # 8) Find maximum score\n",
    "    idx = np.argmax(scores_thresholds)\n",
    "    if return_threshold:\n",
    "        return scores_thresholds[idx], thresholds[idx]\n",
    "    else:\n",
    "        return scores_thresholds[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d608dff-f8ca-4d25-925f-b7d752d052a4",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02da1821-eb37-4730-a30f-e3c8c5ed3a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now at dataset machine-1-1\n",
      "Now at dataset machine-1-2\n",
      "Now at dataset machine-1-3\n",
      "Now at dataset machine-1-4\n",
      "Now at dataset machine-1-5\n"
     ]
    }
   ],
   "source": [
    "results_total = {}\n",
    "for dataset_id in dataset_ids:\n",
    "    print(f\"Now at dataset {dataset_id}\")\n",
    "    results = {}\n",
    "    test_data = pd.read_csv(f'{data_folder_base_path}/{dataset_id}.test.csv')\n",
    "    labels = test_data['is_anomaly'].to_numpy()\n",
    "    if len(np.unique(labels)) == 1:\n",
    "        continue\n",
    "    approaches_paths = [x for x in os.listdir(f'{output_folder_path}/{dataset_id}') if not x.startswith('.') and not x in ['initial_weights', 'artificial_anomalies']]\n",
    "    for approach_path in approaches_paths:\n",
    "        model_id, learning_strategy_id, anomaly_score_id = approach_path.split('-')\n",
    "        if anomaly_score_id == 'confidence_levels':\n",
    "            continue\n",
    "        if model_id not in results.keys():\n",
    "            results[model_id] = {}\n",
    "        if learning_strategy_id not in results[model_id].keys():\n",
    "            results[model_id][learning_strategy_id] = {}\n",
    "        if anomaly_score_id not in results[model_id][learning_strategy_id].keys():\n",
    "            results[model_id][learning_strategy_id][anomaly_score_id] = {}\n",
    "        run_date_id = [x for x in os.listdir(f'{output_folder_path}/{dataset_id}/{approach_path}') if not x.startswith('.')][0]\n",
    "        score_path = f'{output_folder_path}/{dataset_id}/{approach_path}/{run_date_id}'\n",
    "        anomaly_scores = pd.read_csv(f'{score_path}/anomaly_scores.csv').to_numpy()\n",
    "        nonconformity_scores = pd.read_csv(f'{score_path}/nonconformity_scores.csv').to_numpy()\n",
    "        start_idx = int(anomaly_scores[0, 0])\n",
    "        number_of_elements = min(min(len(labels[start_idx:]), len(anomaly_scores)), len(nonconformity_scores))\n",
    "        for scores in [(\"anomaly_scores\", anomaly_scores), (\"nonconformity_scores\", nonconformity_scores)]:\n",
    "            auc_score, pr_threshold, precision, recall = pr_auc(scores[1][:number_of_elements, 1], labels[start_idx:][:number_of_elements])\n",
    "            nab_score, nab_threshold = nab_scoring(scores[1][:number_of_elements, 1], labels[start_idx:][:number_of_elements])\n",
    "            results[model_id][learning_strategy_id][anomaly_score_id][scores[0]] = {\n",
    "                \"pr-auc\": {\n",
    "                    \"threshold\": pr_threshold,\n",
    "                    \"auc\": auc_score,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                },\n",
    "                \"nab\": {\n",
    "                    \"threshold\": nab_threshold,\n",
    "                    \"score\": nab_score,\n",
    "                }\n",
    "            }\n",
    "    results_total[dataset_id] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "753943ad-1116-4a1f-add7-abdbed8060ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'../out/{dataset_category}/{collection_id}/{tuning_job_name}/results_total.json', 'w') as file:\n",
    "    json.dump(results_total, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddf2b8-9bac-4666-890b-e8ede6cbff4c",
   "metadata": {},
   "source": [
    "## Post Processing\n",
    "Here we calculate results regarding\n",
    "- ML model + learning strategy performance averaged across collection (for either anomaly likelihood or average)\n",
    "- ML model performance averaged across collection + learning strategies (for either anomaly likelihood or average)\n",
    "- LS performance averaged across collection + ML models (for either anomaly likelihood or average)\n",
    "- Anomaly Likelihood vs Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4f79356-3f98-4b9b-b79a-76ff9fbfa2b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_post_processing = {\n",
    "    'models': {},\n",
    "    'learning_strategies': {},\n",
    "    'models_learning_strategies': {},\n",
    "    'anomaly_scores': {},\n",
    "    'nonconformity_scores': {}\n",
    "}\n",
    "template = {'auc': [], 'precision': [], 'recall': [], 'nab': []}\n",
    "results_post_processing['nonconformity_scores']['default'] = deepcopy(template)\n",
    "for dataset_id in results_total.keys():\n",
    "    approaches_paths = [x for x in os.listdir(f'{output_folder_path}/{dataset_id}') if not x.startswith('.') and not x in ['initial_weights', 'artificial_anomalies']]\n",
    "    for approach_path in approaches_paths:\n",
    "        model_id, learning_strategy_id, anomaly_score_id = approach_path.split('-')\n",
    "        model_ls_id = f'{model_id}-{learning_strategy_id}'\n",
    "        if anomaly_score_id == 'confidence_levels':\n",
    "            continue\n",
    "        if model_id not in results_post_processing['models'].keys():\n",
    "            results_post_processing['models'][model_id] = deepcopy(template)\n",
    "        if learning_strategy_id not in results_post_processing['learning_strategies'].keys():\n",
    "            results_post_processing['learning_strategies'][learning_strategy_id] = deepcopy(template)\n",
    "        if model_ls_id not in results_post_processing['models_learning_strategies'].keys():\n",
    "            results_post_processing['models_learning_strategies'][model_ls_id] = deepcopy(template)\n",
    "        if anomaly_score_id not in results_post_processing['anomaly_scores'].keys():\n",
    "            results_post_processing['anomaly_scores'][anomaly_score_id] = deepcopy(template)\n",
    "        for dict_key, obj_id in [('models', model_id), ('learning_strategies', learning_strategy_id), ('models_learning_strategies', model_ls_id), ('anomaly_scores', anomaly_score_id)]:\n",
    "            for pr_key in ['auc', 'precision', 'recall']:\n",
    "                results_post_processing[dict_key][obj_id][pr_key].append(results_total[dataset_id][model_id][learning_strategy_id][anomaly_score_id]['anomaly_scores']['pr-auc'][pr_key])\n",
    "                results_post_processing['nonconformity_scores']['default'][pr_key].append(results_total[dataset_id][model_id][learning_strategy_id][anomaly_score_id]['nonconformity_scores']['pr-auc'][pr_key])\n",
    "            results_post_processing[dict_key][obj_id]['nab'].append(results_total[dataset_id][model_id][learning_strategy_id][anomaly_score_id]['anomaly_scores']['nab']['score'])\n",
    "            results_post_processing['nonconformity_scores']['default']['nab'].append(results_total[dataset_id][model_id][learning_strategy_id][anomaly_score_id]['nonconformity_scores']['nab']['score'])\n",
    "            \n",
    "# Average across categories\n",
    "for dict_key in results_post_processing.keys():\n",
    "    for obj_key in results_post_processing[dict_key].keys():\n",
    "        for score_key in template.keys():\n",
    "            results_post_processing[dict_key][obj_key][score_key] = sum(results_post_processing[dict_key][obj_key][score_key]) / max(1, len(results_post_processing[dict_key][obj_key][score_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e23c4a3-e417-4dd1-b153-31e474fa32c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'../out/{dataset_category}/{collection_id}/{tuning_job_name}/results_post_processing.json', 'w') as file:\n",
    "    json.dump(results_post_processing, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68e527-bc1b-4e4a-a627-d658c0ce440d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
